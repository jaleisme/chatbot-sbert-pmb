{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah pertanyaan:  (259, 2)\n",
      "Jumlah pertanyaan:  (17, 2)\n"
     ]
    }
   ],
   "source": [
    "question_path = './data/dataset-question.csv'\n",
    "answer_path = './data/dataset-answer.csv'\n",
    "\n",
    "question_attr = ['pattern', 'tag']\n",
    "answer_attr = ['responses', 'tag']\n",
    "\n",
    "question_data = pd.read_csv(question_path)[question_attr]\n",
    "answer_data = pd.read_csv(answer_path)[answer_attr]\n",
    "\n",
    "# data_answer.head(10)\n",
    "print(\"Jumlah pertanyaan: \", question_data.shape)\n",
    "print(\"Jumlah pertanyaan: \", answer_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi</td>\n",
       "      <td>gr_hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Halo</td>\n",
       "      <td>gr_ha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Selamat Pagi</td>\n",
       "      <td>gr_pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Selamat Siang</td>\n",
       "      <td>gr_si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Selamat Sore</td>\n",
       "      <td>gr_so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Selamat Malam</td>\n",
       "      <td>gr_ma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ada program studi apa saja di UIN Sunan Gunung...</td>\n",
       "      <td>prodi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apa saja jurusan di UIN Bandung</td>\n",
       "      <td>prodi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ada jurusan apa saja?</td>\n",
       "      <td>prodi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mohon informasi mengenai program studi yang te...</td>\n",
       "      <td>prodi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pattern    tag\n",
       "0                                                 Hi  gr_hi\n",
       "1                                               Halo  gr_ha\n",
       "2                                       Selamat Pagi  gr_pa\n",
       "3                                      Selamat Siang  gr_si\n",
       "4                                       Selamat Sore  gr_so\n",
       "5                                      Selamat Malam  gr_ma\n",
       "6  Ada program studi apa saja di UIN Sunan Gunung...  prodi\n",
       "7                    Apa saja jurusan di UIN Bandung  prodi\n",
       "8                              Ada jurusan apa saja?  prodi\n",
       "9  Mohon informasi mengenai program studi yang te...  prodi"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responses</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Halo! Apa yang ingin anda cari tahu hari ini?</td>\n",
       "      <td>gr_hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi! Apa yang bisa kami bantu?</td>\n",
       "      <td>gr_ha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Selamat Pagi! Apa yang bisa kami bantu untuk m...</td>\n",
       "      <td>gr_pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Selamat Siang! Apa yang ingin anda cari tahu?</td>\n",
       "      <td>gr_si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Selamat Sore! Apa yang ingin anda ketahui?</td>\n",
       "      <td>gr_so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Selamat Malam! Apa yang bisa kami bantu?</td>\n",
       "      <td>gr_ma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UIN Sunan Gunung Djati Bandung menyelenggaraka...</td>\n",
       "      <td>prodi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNBP adalah singkatan dari Seleksi Nasional Be...</td>\n",
       "      <td>jm_s1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Untuk jurusan yang bisa dipilih pada Seleksi M...</td>\n",
       "      <td>snbp_jur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Untuk jurusan yang bisa dipilih pada Seleksi M...</td>\n",
       "      <td>snbt_jur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           responses       tag\n",
       "0      Halo! Apa yang ingin anda cari tahu hari ini?     gr_hi\n",
       "1                      Hi! Apa yang bisa kami bantu?     gr_ha\n",
       "2  Selamat Pagi! Apa yang bisa kami bantu untuk m...     gr_pa\n",
       "3      Selamat Siang! Apa yang ingin anda cari tahu?     gr_si\n",
       "4         Selamat Sore! Apa yang ingin anda ketahui?     gr_so\n",
       "5           Selamat Malam! Apa yang bisa kami bantu?     gr_ma\n",
       "6  UIN Sunan Gunung Djati Bandung menyelenggaraka...     prodi\n",
       "7  SNBP adalah singkatan dari Seleksi Nasional Be...     jm_s1\n",
       "8  Untuk jurusan yang bisa dipilih pada Seleksi M...  snbp_jur\n",
       "9  Untuk jurusan yang bisa dipilih pada Seleksi M...  snbt_jur"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: transformers in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (4.42.4)\n",
      "Requirement already satisfied: tensorflow in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (2.16.2)\n",
      "Requirement already satisfied: filelock in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (4.12.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.2->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install Sastrawi transformers tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT_TOKENIZER = 'firqaaa/indo-sentence-bert-base'\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, max_len=128):\n",
    "        self.stemmer = StemmerFactory().create_stemmer()\n",
    "        self.stopword = StopWordRemoverFactory().create_stop_word_remover()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(SBERT_TOKENIZER)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def cleaning(self, val):\n",
    "        val = re.sub(r'\\s+', ' ', val)\n",
    "        val = re.sub(\"[^a-zA-Z0-9;]\", \" \", val)\n",
    "        return val\n",
    "    \n",
    "    def casefolding(self, val):\n",
    "        return str(val).lower()\n",
    "    \n",
    "    def stemming(self, val):\n",
    "        return self.stemmer.stem(str(val))\n",
    "    \n",
    "    def stopwordRemoval(self, val):\n",
    "        return self.stopword.remove(str(val))\n",
    "    \n",
    "    def embedding(self, val):\n",
    "        return self.tokenizer.encode_plus(\n",
    "            val,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "    \n",
    "    def preprocessing(self, sentences):\n",
    "        for i in range(len(sentences)):\n",
    "            input = sentences[i]\n",
    "            input = self.cleaning(input)\n",
    "            input = self.casefolding(input)\n",
    "            input = self.stemming(input)\n",
    "            input = self.stopwordremove(input)\n",
    "            sentences[i] = input\n",
    "        return sentences\n",
    "\n",
    "    def tokenizing(self, sentences):\n",
    "        input_ids, attention_mask = [], []\n",
    "        for sentence in sentences:\n",
    "            output = self.embedding(sentence)\n",
    "            input_ids.append(output['input_ids'])\n",
    "            attention_mask.append(output['attention_mask'])\n",
    "        return {\n",
    "            'input_ids': tf.convert_to_tensor(\n",
    "                np.asarray(input_ids).squeeze(),\n",
    "                dtype=tf.int32\n",
    "            ),\n",
    "            'attention_mask': tf.convert_to_tensor(\n",
    "                np.asarray(attention_mask).squeeze(),\n",
    "                dtype=tf.int32\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def preprocess_get_token(self, sentences, display_len=20):\n",
    "        preprocessing = self.preprocessing(sentences)\n",
    "        tokenized = self.tokenizing(preprocessing)\n",
    "        return [self.tokenizer.convert_ids_to_tokens(tokenized['input_ids'][i][:display_len]) for i in range(len(sentences))]\n",
    "\n",
    "preprocess = Preprocess()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casefolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apa saja jurusan di UIN Bandung\n",
      "Ada jurusan apa saja?\n",
      "Mohon informasi mengenai program studi yang tersedia di UIN Sunan Gunung Djati Bandung.\n"
     ]
    }
   ],
   "source": [
    "sample_texts = question_data['pattern']\n",
    "print(sample_texts[7])\n",
    "print(sample_texts[8])\n",
    "print(sample_texts[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before Casefolding</th>\n",
       "      <th>After Casefolding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Ada beasiswa di UIN Sunan Gunung Djati Bandung?</td>\n",
       "      <td>ada beasiswa di uin sunan gunung djati bandung?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Beasiswa ada di UIN Sunan Gunung Djati Bandung...</td>\n",
       "      <td>beasiswa ada di uin sunan gunung djati bandung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Di UIN Sunan Gunung Djati Bandung ada beasiswa...</td>\n",
       "      <td>di uin sunan gunung djati bandung ada beasiswa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Beasiswa di UIN Sunan Gunung Djati Bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>UIN Sunan Gunung Djati Bandung punya beasiswa?</td>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Ada info soal beasiswa di UIN Sunan Gunung Dja...</td>\n",
       "      <td>ada info soal beasiswa di uin sunan gunung dja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Beasiswa di UIN Sunan Gunung Djati Bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Minta info dong, di UIN Sunan Gunung Djati Ban...</td>\n",
       "      <td>minta info dong, di uin sunan gunung djati ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Beasiswa di UIN Sunan Gunung Djati Bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>UIN Sunan Gunung Djati Bandung ada beasiswa?</td>\n",
       "      <td>uin sunan gunung djati bandung ada beasiswa?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Before Casefolding  \\\n",
       "249    Ada beasiswa di UIN Sunan Gunung Djati Bandung?   \n",
       "250  Beasiswa ada di UIN Sunan Gunung Djati Bandung...   \n",
       "251  Di UIN Sunan Gunung Djati Bandung ada beasiswa...   \n",
       "252  Beasiswa di UIN Sunan Gunung Djati Bandung ada...   \n",
       "253     UIN Sunan Gunung Djati Bandung punya beasiswa?   \n",
       "254  Ada info soal beasiswa di UIN Sunan Gunung Dja...   \n",
       "255  Beasiswa di UIN Sunan Gunung Djati Bandung ada...   \n",
       "256  Minta info dong, di UIN Sunan Gunung Djati Ban...   \n",
       "257  Beasiswa di UIN Sunan Gunung Djati Bandung ada...   \n",
       "258       UIN Sunan Gunung Djati Bandung ada beasiswa?   \n",
       "\n",
       "                                     After Casefolding  \n",
       "249    ada beasiswa di uin sunan gunung djati bandung?  \n",
       "250  beasiswa ada di uin sunan gunung djati bandung...  \n",
       "251  di uin sunan gunung djati bandung ada beasiswa...  \n",
       "252  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "253     uin sunan gunung djati bandung punya beasiswa?  \n",
       "254  ada info soal beasiswa di uin sunan gunung dja...  \n",
       "255  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "256  minta info dong, di uin sunan gunung djati ban...  \n",
       "257  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "258       uin sunan gunung djati bandung ada beasiswa?  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = Preprocess()\n",
    "hasil_casefolding = [preprocess.casefolding(sample) for sample in sample_texts]\n",
    "\n",
    "bf_cf = sample_texts\n",
    "af_cf = hasil_casefolding\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Before Casefolding': bf_cf,\n",
    "    'After Casefolding': af_cf \n",
    "})\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before Cleaning</th>\n",
       "      <th>After Cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>ada beasiswa di uin sunan gunung djati bandung?</td>\n",
       "      <td>ada beasiswa di uin sunan gunung djati bandung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>beasiswa ada di uin sunan gunung djati bandung...</td>\n",
       "      <td>beasiswa ada di uin sunan gunung djati bandung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>di uin sunan gunung djati bandung ada beasiswa...</td>\n",
       "      <td>di uin sunan gunung djati bandung ada beasiswa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa?</td>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>ada info soal beasiswa di uin sunan gunung dja...</td>\n",
       "      <td>ada info soal beasiswa di uin sunan gunung dja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>minta info dong, di uin sunan gunung djati ban...</td>\n",
       "      <td>minta info dong  di uin sunan gunung djati ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>uin sunan gunung djati bandung ada beasiswa?</td>\n",
       "      <td>uin sunan gunung djati bandung ada beasiswa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Before Cleaning  \\\n",
       "249    ada beasiswa di uin sunan gunung djati bandung?   \n",
       "250  beasiswa ada di uin sunan gunung djati bandung...   \n",
       "251  di uin sunan gunung djati bandung ada beasiswa...   \n",
       "252  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "253     uin sunan gunung djati bandung punya beasiswa?   \n",
       "254  ada info soal beasiswa di uin sunan gunung dja...   \n",
       "255  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "256  minta info dong, di uin sunan gunung djati ban...   \n",
       "257  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "258       uin sunan gunung djati bandung ada beasiswa?   \n",
       "\n",
       "                                        After Cleaning  \n",
       "249    ada beasiswa di uin sunan gunung djati bandung   \n",
       "250  beasiswa ada di uin sunan gunung djati bandung...  \n",
       "251  di uin sunan gunung djati bandung ada beasiswa...  \n",
       "252  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "253     uin sunan gunung djati bandung punya beasiswa   \n",
       "254  ada info soal beasiswa di uin sunan gunung dja...  \n",
       "255  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "256  minta info dong  di uin sunan gunung djati ban...  \n",
       "257  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "258       uin sunan gunung djati bandung ada beasiswa   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = Preprocess()\n",
    "hasil_cleaning = [preprocess.cleaning(sample) for sample in hasil_casefolding]\n",
    "bf_cl = hasil_casefolding\n",
    "af_cl = hasil_cleaning\n",
    "df = pd.DataFrame({\n",
    "    'Before Cleaning': bf_cl,\n",
    "    'After Cleaning': af_cl\n",
    "})\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before Stemming</th>\n",
       "      <th>After Stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>ada beasiswa di uin sunan gunung djati bandung</td>\n",
       "      <td>ada beasiswa di uin sunan gunung djati bandung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>beasiswa ada di uin sunan gunung djati bandung...</td>\n",
       "      <td>beasiswa ada di uin sunan gunung djati bandung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>di uin sunan gunung djati bandung ada beasiswa...</td>\n",
       "      <td>di uin sunan gunung djati bandung ada beasiswa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa</td>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>ada info soal beasiswa di uin sunan gunung dja...</td>\n",
       "      <td>ada info soal beasiswa di uin sunan gunung dja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>minta info dong  di uin sunan gunung djati ban...</td>\n",
       "      <td>minta info dong di uin sunan gunung djati band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>uin sunan gunung djati bandung ada beasiswa</td>\n",
       "      <td>uin sunan gunung djati bandung ada beasiswa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Before Stemming  \\\n",
       "249    ada beasiswa di uin sunan gunung djati bandung    \n",
       "250  beasiswa ada di uin sunan gunung djati bandung...   \n",
       "251  di uin sunan gunung djati bandung ada beasiswa...   \n",
       "252  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "253     uin sunan gunung djati bandung punya beasiswa    \n",
       "254  ada info soal beasiswa di uin sunan gunung dja...   \n",
       "255  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "256  minta info dong  di uin sunan gunung djati ban...   \n",
       "257  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "258       uin sunan gunung djati bandung ada beasiswa    \n",
       "\n",
       "                                        After Stemming  \n",
       "249     ada beasiswa di uin sunan gunung djati bandung  \n",
       "250  beasiswa ada di uin sunan gunung djati bandung...  \n",
       "251  di uin sunan gunung djati bandung ada beasiswa...  \n",
       "252  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "253      uin sunan gunung djati bandung punya beasiswa  \n",
       "254  ada info soal beasiswa di uin sunan gunung dja...  \n",
       "255  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "256  minta info dong di uin sunan gunung djati band...  \n",
       "257  beasiswa di uin sunan gunung djati bandung ada...  \n",
       "258        uin sunan gunung djati bandung ada beasiswa  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = Preprocess()\n",
    "hasil_stemming = [preprocess.stemming(sample) for sample in hasil_cleaning]\n",
    "bf_st = hasil_cleaning\n",
    "af_st = hasil_stemming\n",
    "df = pd.DataFrame({\n",
    "    \"Before Stemming\" : bf_st,\n",
    "    \"After Stemming\" : af_st\n",
    "})\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before Stopword Removal</th>\n",
       "      <th>After Stopword Removal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>ada beasiswa di uin sunan gunung djati bandung</td>\n",
       "      <td>beasiswa uin sunan gunung djati bandung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>beasiswa ada di uin sunan gunung djati bandung...</td>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>di uin sunan gunung djati bandung ada beasiswa...</td>\n",
       "      <td>uin sunan gunung djati bandung beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa uin sunan gunung djati bandung nggak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa</td>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>ada info soal beasiswa di uin sunan gunung dja...</td>\n",
       "      <td>info soal beasiswa uin sunan gunung djati bandung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa uin sunan gunung djati bandung info l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>minta info dong di uin sunan gunung djati band...</td>\n",
       "      <td>minta info dong uin sunan gunung djati bandung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung ada...</td>\n",
       "      <td>beasiswa uin sunan gunung djati bandung gak sih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>uin sunan gunung djati bandung ada beasiswa</td>\n",
       "      <td>uin sunan gunung djati bandung beasiswa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Before Stopword Removal  \\\n",
       "249     ada beasiswa di uin sunan gunung djati bandung   \n",
       "250  beasiswa ada di uin sunan gunung djati bandung...   \n",
       "251  di uin sunan gunung djati bandung ada beasiswa...   \n",
       "252  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "253      uin sunan gunung djati bandung punya beasiswa   \n",
       "254  ada info soal beasiswa di uin sunan gunung dja...   \n",
       "255  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "256  minta info dong di uin sunan gunung djati band...   \n",
       "257  beasiswa di uin sunan gunung djati bandung ada...   \n",
       "258        uin sunan gunung djati bandung ada beasiswa   \n",
       "\n",
       "                                After Stopword Removal  \n",
       "249            beasiswa uin sunan gunung djati bandung  \n",
       "250         beasiswa di uin sunan gunung djati bandung  \n",
       "251            uin sunan gunung djati bandung beasiswa  \n",
       "252      beasiswa uin sunan gunung djati bandung nggak  \n",
       "253      uin sunan gunung djati bandung punya beasiswa  \n",
       "254  info soal beasiswa uin sunan gunung djati bandung  \n",
       "255  beasiswa uin sunan gunung djati bandung info l...  \n",
       "256  minta info dong uin sunan gunung djati bandung...  \n",
       "257    beasiswa uin sunan gunung djati bandung gak sih  \n",
       "258            uin sunan gunung djati bandung beasiswa  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess = Preprocess()\n",
    "hasil_stopword_removal = [preprocess.stopwordRemoval(sample) for sample in hasil_stemming]\n",
    "bf_sr = hasil_stemming\n",
    "af_sr = hasil_stopword_removal\n",
    "df = pd.DataFrame({\n",
    "    \"Before Stopword Removal\": bf_sr,\n",
    "    \"After Stopword Removal\": af_sr\n",
    "})\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering (Drop Duplicate Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data (247, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>mohon klarifikasi ada beasiswa tawar uin sunan...</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>uin sunan gunung djati bandung beasiswa</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung nggak</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>info soal beasiswa uin sunan gunung djati bandung</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung info l...</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>minta info dong uin sunan gunung djati bandung...</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung gak sih</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               pattern       tag\n",
       "248  mohon klarifikasi ada beasiswa tawar uin sunan...  beasiswa\n",
       "249            beasiswa uin sunan gunung djati bandung  beasiswa\n",
       "250         beasiswa di uin sunan gunung djati bandung  beasiswa\n",
       "251            uin sunan gunung djati bandung beasiswa  beasiswa\n",
       "252      beasiswa uin sunan gunung djati bandung nggak  beasiswa\n",
       "253      uin sunan gunung djati bandung punya beasiswa  beasiswa\n",
       "254  info soal beasiswa uin sunan gunung djati bandung  beasiswa\n",
       "255  beasiswa uin sunan gunung djati bandung info l...  beasiswa\n",
       "256  minta info dong uin sunan gunung djati bandung...  beasiswa\n",
       "257    beasiswa uin sunan gunung djati bandung gak sih  beasiswa"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_filtered = pd.DataFrame({\n",
    "    \"pattern\": hasil_stopword_removal,\n",
    "    \"tag\": question_data['tag']\n",
    "})\n",
    "to_be_filtered.head(10)\n",
    "hasil_filtering = to_be_filtered.drop_duplicates(subset=['pattern'])\n",
    "\n",
    "hasil_filtering.to_csv('data/preprocessed-data.csv', index=False)\n",
    "\n",
    "print('Filtered data', hasil_filtering.shape)\n",
    "hasil_filtering.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data:  (247, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>mohon klarifikasi ada beasiswa tawar uin sunan...</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>beasiswa di uin sunan gunung djati bandung</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>uin sunan gunung djati bandung beasiswa</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung nggak</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>info soal beasiswa uin sunan gunung djati bandung</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung info l...</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>minta info dong uin sunan gunung djati bandung...</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung gak sih</td>\n",
       "      <td>beasiswa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               pattern       tag\n",
       "237  mohon klarifikasi ada beasiswa tawar uin sunan...  beasiswa\n",
       "238            beasiswa uin sunan gunung djati bandung  beasiswa\n",
       "239         beasiswa di uin sunan gunung djati bandung  beasiswa\n",
       "240            uin sunan gunung djati bandung beasiswa  beasiswa\n",
       "241      beasiswa uin sunan gunung djati bandung nggak  beasiswa\n",
       "242      uin sunan gunung djati bandung punya beasiswa  beasiswa\n",
       "243  info soal beasiswa uin sunan gunung djati bandung  beasiswa\n",
       "244  beasiswa uin sunan gunung djati bandung info l...  beasiswa\n",
       "245  minta info dong uin sunan gunung djati bandung...  beasiswa\n",
       "246    beasiswa uin sunan gunung djati bandung gak sih  beasiswa"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/preprocessed-data.csv')\n",
    "print(\"Total data: \", dataset.shape)\n",
    "dataset.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining labels\n",
    "# label_list = dataset['tag'].drop_duplicates()\n",
    "# LABELS = label_list.tolist()\n",
    "# len(LABELS), LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Give 0 as initial value\n",
    "# labelled_data = dataset.copy()\n",
    "# labelled_data[LABELS] = 0\n",
    "# labels = pd.Series(label_list)\n",
    "\n",
    "# for i, row in labelled_data.iterrows():\n",
    "#     tag = row['tag'].split(';')\n",
    "#     labelled_data.loc[i, tag] = 1\n",
    "# labelled_data.tail(1)\n",
    "# # labelled_data[LABELS].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# sentences = []\n",
    "# for sentence in dataset['pattern']:\n",
    "#     sentences.append(sentence)\n",
    "\n",
    "# model = SentenceTransformer('firqaaa/indo-sentence-bert-base')\n",
    "# embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def low_confident(indexes):\n",
    "#     tags = []\n",
    "#     data = dataset.to_numpy().tolist()\n",
    "#     pos = 0\n",
    "#     for tag in data:\n",
    "#         for id in indexes:\n",
    "#             if(id == pos):\n",
    "#                 tags.append(tag)\n",
    "#         pos += 1\n",
    "#     tags = pd.DataFrame(tags)\n",
    "#     tags = tags.drop_duplicates()\n",
    "#     return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asked_quetion = 'Informasi seputar biaya UKT di UIN Sunan Gunung Djati Bandung'\n",
    "# encoded_question = model.encode(asked_quetion)\n",
    "# result = np.array(model.similarity(encoded_question, embeddings))\n",
    "# highestIndex = np.argmax(result)\n",
    "# highestVal = np.max(result)\n",
    "# tag = dataset.loc[highestIndex]['tag']\n",
    "\n",
    "# print\n",
    "# highestIndex, highestVal, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get confidents higher than 0.5\n",
    "# confidents = []\n",
    "# indexes = []\n",
    "# pos = 0\n",
    "\n",
    "# for item in result:\n",
    "#     for value in item:\n",
    "#         if(value >= 0.8):\n",
    "#             confidents.append(value)\n",
    "#             indexes.append(pos)\n",
    "#         elif(value < 0.8):\n",
    "#             recommended_tags = low_confident(indexes)\n",
    "#             recommended_tags\n",
    "#         pos += 1\n",
    "# confidents, indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "\n",
    "# #Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# # Sentences we want sentence embeddings for\n",
    "# sentences = [\"Ibukota Perancis adalah Paris\", \n",
    "#              \"Menara Eifel terletak di Paris, Perancis\", \n",
    "#              \"Pizza adalah makanan khas Italia\", \n",
    "#              \"Saya kuliah di Carneige Mellon University\"]\n",
    "\n",
    "\n",
    "# # Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('firqaaa/indo-sentence-bert-base')\n",
    "# model = AutoModel.from_pretrained('firqaaa/indo-sentence-bert-base')\n",
    "\n",
    "# # Tokenize sentences\n",
    "# encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Perform pooling. In this case, mean pooling.\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# print(\"Sentence embeddings:\")\n",
    "# print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (0.32.1)\n",
      "Requirement already satisfied: datasets in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: sentence-transformers[train] in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers[torch] in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (4.42.4)\n",
      "Requirement already satisfied: tqdm in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from sentence-transformers[train]) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from sentence-transformers[train]) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from sentence-transformers[train]) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from sentence-transformers[train]) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from sentence-transformers[train]) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from sentence-transformers[train]) (0.23.2)\n",
      "Requirement already satisfied: Pillow in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from sentence-transformers[train]) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers[torch]) (3.14.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers[torch]) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: psutil in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers[train]) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2024.6.2)\n",
      "Requirement already satisfied: sympy in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers[train]) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers[train]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers[train]) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers[train]) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from tqdm->sentence-transformers[train]) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers[train]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers[train]) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers[train]) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers[train]) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers[train]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\projects\\uni\\python\\skripsi\\sbert_chatbot\\.venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers[train]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"sentence-transformers[train]\" \" transformers[torch]\" accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments\n",
    ")\n",
    "from sentence_transformers.losses import CoSENTLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = SentenceTransformer(\"firqaaa/indo-sentence-bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 247 examples [00:00, 22000.75 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['pattern', 'tag'],\n",
       "         num_rows: 247\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['pattern', 'tag'],\n",
       "         num_rows: 259\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define datasets\n",
    "train_dataset = load_dataset(\"csv\", data_files=\"data/preprocessed-data.csv\")\n",
    "test_dataset = load_dataset(\"csv\", data_files=\"data/dataset-question.csv\")\n",
    "eval_dataset = load_dataset(\"csv\", data_files=\"data/dataset-eval.csv\")\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (CoSENTLoss | Cosine Sentence Loss -> Returning float similarity score)\n",
    "loss = CoSENTLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training args\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"fine-tuned/sbert-fine-tuned-chatPMB\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0                                                     hi\n",
       " 1                                                   halo\n",
       " 2                                           selamat pagi\n",
       " 3                                          selamat siang\n",
       " 4                                           selamat sore\n",
       "                              ...                        \n",
       " 242        uin sunan gunung djati bandung punya beasiswa\n",
       " 243    info soal beasiswa uin sunan gunung djati bandung\n",
       " 244    beasiswa uin sunan gunung djati bandung info l...\n",
       " 245    minta info dong uin sunan gunung djati bandung...\n",
       " 246      beasiswa uin sunan gunung djati bandung gak sih\n",
       " Name: pattern, Length: 247, dtype: object,\n",
       " 0                                                     Hi\n",
       " 1                                                   Halo\n",
       " 2                                           Selamat Pagi\n",
       " 3                                          Selamat Siang\n",
       " 4                                           Selamat Sore\n",
       "                              ...                        \n",
       " 242    Bisakah Anda memberitahu saya apakah di UIN Su...\n",
       " 243    Dapatkah Anda memberikan informasi tentang ada...\n",
       " 244    Mohon penjelasan apakah ada program beasiswa d...\n",
       " 245    Saya ingin menanyakan apakah di UIN Sunan Gunu...\n",
       " 246    Mohon konfirmasi apakah UIN Sunan Gunung Djati...\n",
       " Name: pattern, Length: 247, dtype: object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating eval dataset\n",
    "train_patterns = pd.read_csv('data/preprocessed-data.csv')['pattern']\n",
    "test_patterns = pd.read_csv('data/dataset-question.csv')['pattern']\n",
    "test_patterns = test_patterns.iloc[:-12]\n",
    "train_patterns, test_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train = model.encode(train_patterns)\n",
    "embed_test = model.encode(test_patterns)\n",
    "# embed_test, embed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_patterns</th>\n",
       "      <th>test_patterns</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi</td>\n",
       "      <td>Hi</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halo</td>\n",
       "      <td>Halo</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>selamat pagi</td>\n",
       "      <td>Selamat Pagi</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selamat siang</td>\n",
       "      <td>Selamat Siang</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selamat sore</td>\n",
       "      <td>Selamat Sore</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>uin sunan gunung djati bandung punya beasiswa</td>\n",
       "      <td>Bisakah Anda memberitahu saya apakah di UIN Su...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>info soal beasiswa uin sunan gunung djati bandung</td>\n",
       "      <td>Dapatkah Anda memberikan informasi tentang ada...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung info l...</td>\n",
       "      <td>Mohon penjelasan apakah ada program beasiswa d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>minta info dong uin sunan gunung djati bandung...</td>\n",
       "      <td>Saya ingin menanyakan apakah di UIN Sunan Gunu...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>beasiswa uin sunan gunung djati bandung gak sih</td>\n",
       "      <td>Mohon konfirmasi apakah UIN Sunan Gunung Djati...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        train_patterns  \\\n",
       "0                                                   hi   \n",
       "1                                                 halo   \n",
       "2                                         selamat pagi   \n",
       "3                                        selamat siang   \n",
       "4                                         selamat sore   \n",
       "..                                                 ...   \n",
       "242      uin sunan gunung djati bandung punya beasiswa   \n",
       "243  info soal beasiswa uin sunan gunung djati bandung   \n",
       "244  beasiswa uin sunan gunung djati bandung info l...   \n",
       "245  minta info dong uin sunan gunung djati bandung...   \n",
       "246    beasiswa uin sunan gunung djati bandung gak sih   \n",
       "\n",
       "                                         test_patterns  score  \n",
       "0                                                   Hi    1.0  \n",
       "1                                                 Halo    1.0  \n",
       "2                                         Selamat Pagi    1.0  \n",
       "3                                        Selamat Siang    1.0  \n",
       "4                                         Selamat Sore    1.0  \n",
       "..                                                 ...    ...  \n",
       "242  Bisakah Anda memberitahu saya apakah di UIN Su...    1.0  \n",
       "243  Dapatkah Anda memberikan informasi tentang ada...    1.0  \n",
       "244  Mohon penjelasan apakah ada program beasiswa d...    1.0  \n",
       "245  Saya ingin menanyakan apakah di UIN Sunan Gunu...    1.0  \n",
       "246  Mohon konfirmasi apakah UIN Sunan Gunung Djati...    1.0  \n",
       "\n",
       "[247 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "temp = 0\n",
    "result = np.array(model.similarity(embed_train, embed_test))\n",
    "# result\n",
    "for val in result:\n",
    "    for data in val:\n",
    "        if(temp<data):\n",
    "            temp=data\n",
    "    scores.append(temp)\n",
    "dev = pd.DataFrame({\n",
    "    \"train_patterns\": train_patterns,\n",
    "    \"test_patterns\": test_patterns,\n",
    "    \"score\": scores\n",
    "})\n",
    "# dev.to_csv('data/dataset-eval.csv')\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# sns.heatmap(\n",
    "#     result,\n",
    "#     annot=True,\n",
    "#     fmt=\"d\",\n",
    "#     cmap=\"Blues\",\n",
    "#     xticklabels=train_patterns,\n",
    "#     yticklabels=test_patterns,\n",
    "# )\n",
    "# plt.xlabel(\"Predicted labels\")\n",
    "# plt.ylabel(\"True labels\")\n",
    "# plt.title(\"Training Data\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sts-dev_pearson_cosine': 0.6937476450106933,\n",
       " 'sts-dev_spearman_cosine': 0.11990915114298019,\n",
       " 'sts-dev_pearson_manhattan': 0.6584539015552953,\n",
       " 'sts-dev_spearman_manhattan': 0.1382666264786833,\n",
       " 'sts-dev_pearson_euclidean': 0.6609010834465888,\n",
       " 'sts-dev_spearman_euclidean': 0.13864217644517413,\n",
       " 'sts-dev_pearson_dot': 0.7086632913413077,\n",
       " 'sts-dev_spearman_dot': 0.09030494260026221,\n",
       " 'sts-dev_pearson_max': 0.7086632913413077,\n",
       " 'sts-dev_spearman_max': 0.13864217644517413}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluator & evaluate the base model\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=dev[\"train_patterns\"],\n",
    "    sentences2=dev[\"test_patterns\"],\n",
    "    scores=dev[\"score\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-dev\",\n",
    ")\n",
    "dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 100/1100 [09:48<1:37:06,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9566, 'grad_norm': 127.71992492675781, 'learning_rate': 1.8181818181818182e-05, 'epoch': 9.09}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mdev_evaluator,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2793\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2791\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2793\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m   2796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2750\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   2749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2750\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2753\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\sentence_transformers\\trainer.py:382\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, DatasetDict) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    381\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_dataset_name_column(eval_dataset)\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3623\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3621\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_dataset_name, _eval_dataset \u001b[38;5;129;01min\u001b[39;00m eval_dataset\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m-> 3623\u001b[0m     dataset_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3624\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_eval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43meval_dataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43meval_dataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3628\u001b[0m     metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   3629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\sentence_transformers\\trainer.py:382\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, DatasetDict) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    381\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_dataset_name_column(eval_dataset)\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3641\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3638\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3640\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3641\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3642\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3644\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3645\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3651\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\sentence_transformers\\trainer.py:392\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluation_loop\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    386\u001b[0m     dataloader: DataLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     metric_key_prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EvalLoopOutput:\n\u001b[1;32m--> 392\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;66;03m# If the evaluator is not defined, we can just return the output\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3816\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3813\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3815\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 3816\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3817\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Update the observed num examples\u001b[39;49;00m\n\u001b[0;32m   3818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfind_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3819\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\accelerate\\data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\sentence_transformers\\data_collator.py:37\u001b[0m, in \u001b[0;36mSentenceTransformerDataCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Extract the feature columns\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m---> 37\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m tokenized\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     39\u001b[0m         batch[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:922\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: Union[List[\u001b[38;5;28mstr\u001b[39m], List[Dict], List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m            \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\uni\\python\\skripsi\\SBERT_chatbot\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:154\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts, padding)\u001b[0m\n\u001b[0;32m    152\u001b[0m batch1, batch2 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 154\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    155\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    156\u001b[0m to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine-tuned/models/chatPMB-SBERT-pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sts-dev_pearson_cosine': 0.6937476450106933,\n",
       " 'sts-dev_spearman_cosine': 0.11990915114298019,\n",
       " 'sts-dev_pearson_manhattan': 0.6584539015552953,\n",
       " 'sts-dev_spearman_manhattan': 0.1382666264786833,\n",
       " 'sts-dev_pearson_euclidean': 0.6609010834465888,\n",
       " 'sts-dev_spearman_euclidean': 0.13864217644517413,\n",
       " 'sts-dev_pearson_dot': 0.7086632913413077,\n",
       " 'sts-dev_spearman_dot': 0.09030494260026221,\n",
       " 'sts-dev_pearson_max': 0.7086632913413077,\n",
       " 'sts-dev_spearman_max': 0.13864217644517413}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=dev[\"train_patterns\"],\n",
    "    sentences2=dev[\"test_patterns\"],\n",
    "    scores=dev[\"score\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-dev\",\n",
    ")\n",
    "test_evaluator(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
